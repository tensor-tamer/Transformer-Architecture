{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Design & Architecture"
      ],
      "metadata": {
        "id": "t73f4UQVJSlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuaI_ndHIz3g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer, ViTModel, Wav2Vec2Model, Wav2Vec2Tokenizer\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import librosa\n",
        "\n",
        "class MultimodalTransformer(nn.Module):\n",
        "    def __init__(self, text_model_name, image_model_name, audio_model_name, hidden_dim=768, n_heads=8, n_layers=6):\n",
        "        super(MultimodalTransformer, self).__init__()\n",
        "\n",
        "        # Tokenizers and models for text, image, and audio\n",
        "        self.text_tokenizer = BertTokenizer.from_pretrained(text_model_name)\n",
        "        self.text_model = BertModel.from_pretrained(text_model_name)\n",
        "\n",
        "        self.image_model = ViTModel.from_pretrained(image_model_name)\n",
        "        self.image_transform = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.audio_tokenizer = Wav2Vec2Tokenizer.from_pretrained(audio_model_name)\n",
        "        self.audio_model = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
        "\n",
        "        # Linear projection layers to align dimensions for fusion\n",
        "        self.text_proj = nn.Linear(self.text_model.config.hidden_size, hidden_dim)\n",
        "        self.image_proj = nn.Linear(self.image_model.config.hidden_size, hidden_dim)\n",
        "        self.audio_proj = nn.Linear(self.audio_model.config.hidden_size, hidden_dim)\n",
        "\n",
        "        # Transformer layers for cross-modal fusion\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim, nhead=n_heads, num_encoder_layers=n_layers, num_decoder_layers=n_layers\n",
        "        )\n",
        "\n",
        "        # Final classification layer (this can be changed based on your task)\n",
        "        self.classifier = nn.Linear(hidden_dim, 10)  # Example: 10-class classification task\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Tokenize and get input_ids and attention_mask\n",
        "        encoding = self.text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        # Load and transform the image\n",
        "        image = Image.open(image_path)\n",
        "        return self.image_transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    def preprocess_audio(self, audio_path):\n",
        "        # Load audio and tokenize it\n",
        "        audio, sr = librosa.load(audio_path, sr=16000)\n",
        "        encoding = self.audio_tokenizer(audio, return_tensors=\"pt\", padding=True)\n",
        "        return encoding[\"input_values\"]\n",
        "\n",
        "    def forward(self, text, image_path, audio_path):\n",
        "        # Preprocess the inputs\n",
        "        text_input_ids, text_attention_mask = self.preprocess_text(text)\n",
        "        image_input = self.preprocess_image(image_path)\n",
        "        audio_input = self.preprocess_audio(audio_path)\n",
        "\n",
        "        # Extract features from each modality\n",
        "        text_features = self.text_model(input_ids=text_input_ids, attention_mask=text_attention_mask).last_hidden_state\n",
        "        image_features = self.image_model(pixel_values=image_input).last_hidden_state\n",
        "        audio_features = self.audio_model(input_values=audio_input).last_hidden_state\n",
        "\n",
        "        # Project the features into a common dimension\n",
        "        text_proj = self.text_proj(text_features)\n",
        "        image_proj = self.image_proj(image_features)\n",
        "        audio_proj = self.audio_proj(audio_features)\n",
        "\n",
        "        # Concatenate the projected features along the sequence dimension\n",
        "        multimodal_features = torch.cat((text_proj, image_proj, audio_proj), dim=1)\n",
        "\n",
        "        # Use Transformer layers to fuse the multimodal features\n",
        "        fused_features = self.transformer(multimodal_features, multimodal_features)\n",
        "\n",
        "        # Use the fused features for classification (use the [CLS] token)\n",
        "        logits = self.classifier(fused_features[:, 0, :])\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using Optuna\n"
      ],
      "metadata": {
        "id": "lmHOF7URJYIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import optuna\n",
        "from transformers import BertModel, ViTModel, Wav2Vec2Model\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    # Step 1: Define the hyperparameters to tune\n",
        "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2)\n",
        "\n",
        "    # Step 2: Initialize the model\n",
        "    model = MultimodalTransformerTuner(\n",
        "        text_model_name=\"bert-base-uncased\",\n",
        "        image_model_name=\"google/vit-base-patch16-224\",\n",
        "        audio_model_name=\"facebook/wav2vec2-base-960h\"\n",
        "    )\n",
        "\n",
        "    # Step 3: Define the optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Step 4: Define loss function (CrossEntropyLoss for classification tasks)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Step 5: Training loop (replace with actual training data)\n",
        "    # Dummy data for illustration\n",
        "    text_input = \"This is an example text input.\"\n",
        "    image_path = \"example_image.jpg\"\n",
        "    audio_path = \"example_audio.wav\"\n",
        "\n",
        "    labels = torch.tensor([0])  # Dummy labels for one batch\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(text_input, image_path, audio_path)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Here, you can add more training steps, batch-wise iteration, etc.\n",
        "\n",
        "    # Step 6: Return the performance metric to Optuna\n",
        "    return loss.item()\n",
        "\n",
        "# Run the Optuna study to find the best hyperparameters\n",
        "study = optuna.create_study(direction=\"minimize\")  # Minimize the loss\n",
        "study.optimize(objective, n_trials=10)  # You can adjust the number of trials\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(f\"Best hyperparameters: {best_params}\")"
      ],
      "metadata": {
        "id": "ecyBx7owJIg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize final model with the best hyperparameters\n",
        "final_model = MultimodalTransformerTuner(\n",
        "    text_model_name=\"bert-base-uncased\",\n",
        "    image_model_name=\"google/vit-base-patch16-224\",\n",
        "    audio_model_name=\"facebook/wav2vec2-base-960h\"\n",
        ")\n",
        "\n",
        "# Use best hyperparameters for optimizer\n",
        "final_optimizer = optim.AdamW(final_model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "\n",
        "# Train the model with the optimal settings."
      ],
      "metadata": {
        "id": "v8jL07sNJRHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}